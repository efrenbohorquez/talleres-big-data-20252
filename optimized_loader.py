"""
MÃ³dulo optimizado para cargas masivas de JSON a MongoDB
Implementa mejores prÃ¡cticas para grandes volÃºmenes de datos
"""

import os
import sys
import logging
from typing import List, Dict, Any, Generator
from datetime import datetime
import json
from pymongo import MongoClient
from pymongo.errors import BulkWriteError
from tqdm import tqdm
import time

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
from src.mongodb_connection import mongo_connection

class OptimizedBulkLoader:
    """Cargador optimizado para grandes volÃºmenes de datos JSON"""
    
    def __init__(self, batch_size: int = 1000):
        """
        Inicializa el cargador optimizado
        
        Args:
            batch_size (int): TamaÃ±o de lote para inserciÃ³n bulk (default: 1000)
        """
        self.batch_size = batch_size
        self.logger = logging.getLogger(__name__)
        self.stats = {
            "total_processed": 0,
            "total_inserted": 0,
            "total_errors": 0,
            "batches_processed": 0,
            "start_time": None,
            "end_time": None
        }
    
    def chunk_documents(self, documents: List[Dict[str, Any]]) -> Generator[List[Dict[str, Any]], None, None]:
        """
        Divide una lista de documentos en lotes
        
        Args:
            documents: Lista de documentos a dividir
            
        Yields:
            Lotes de documentos del tamaÃ±o especificado
        """
        for i in range(0, len(documents), self.batch_size):
            yield documents[i:i + self.batch_size]
    
    def bulk_insert_optimized(self, documents: List[Dict[str, Any]], 
                            ordered: bool = False, 
                            bypass_validation: bool = True) -> Dict[str, Any]:
        """
        Inserta documentos en lotes optimizados
        
        Args:
            documents: Lista de documentos a insertar
            ordered: Si mantener el orden (False = mÃ¡s rÃ¡pido)
            bypass_validation: Saltar validaciÃ³n de esquema (mÃ¡s rÃ¡pido)
            
        Returns:
            Resultado de la operaciÃ³n bulk
        """
        if not documents:
            return {"inserted_count": 0, "errors": []}
        
        self.stats["start_time"] = datetime.now()
        
        try:
            if mongo_connection.collection is None:
                raise Exception("No hay conexiÃ³n a MongoDB")
            
            total_inserted = 0
            errors = []
            
            # Procesar en lotes
            for batch in tqdm(self.chunk_documents(documents), 
                            desc="Insertando lotes", 
                            total=(len(documents) + self.batch_size - 1) // self.batch_size):
                
                try:
                    # Configurar opciones de inserciÃ³n optimizada
                    options = {
                        "ordered": ordered,
                        "bypass_document_validation": bypass_validation
                    }
                    
                    result = mongo_connection.collection.insert_many(batch, **options)
                    total_inserted += len(result.inserted_ids)
                    self.stats["batches_processed"] += 1
                    
                    self.logger.info(f"Lote insertado: {len(result.inserted_ids)} documentos")
                    
                except BulkWriteError as bwe:
                    # Manejar errores parciales
                    total_inserted += bwe.details.get('nInserted', 0)
                    errors.extend(bwe.details.get('writeErrors', []))
                    self.logger.warning(f"Errores en lote: {len(bwe.details.get('writeErrors', []))}")
                
                except Exception as e:
                    errors.append({"error": str(e), "batch_size": len(batch)})
                    self.logger.error(f"Error en lote: {e}")
            
            self.stats["total_inserted"] = total_inserted
            self.stats["total_errors"] = len(errors)
            self.stats["end_time"] = datetime.now()
            
            return {
                "inserted_count": total_inserted,
                "errors": errors,
                "batches_processed": self.stats["batches_processed"]
            }
            
        except Exception as e:
            self.logger.error(f"Error en bulk insert: {e}")
            return {"inserted_count": 0, "errors": [str(e)]}
    
    def load_json_files_optimized(self, file_paths: List[str]) -> Dict[str, Any]:
        """
        Carga mÃºltiples archivos JSON de forma optimizada
        
        Args:
            file_paths: Lista de rutas a archivos JSON
            
        Returns:
            Resultado de la carga
        """
        documents = []
        
        print(f"ğŸ”„ Cargando {len(file_paths)} archivos JSON...")
        
        for file_path in tqdm(file_paths, desc="Leyendo archivos"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    # Intentar cargar como JSON
                    try:
                        data = json.load(f)
                        
                        # Si es una lista de objetos JSON
                        if isinstance(data, list):
                            for item in data:
                                if isinstance(item, dict):
                                    item.update({
                                        "source_file": os.path.basename(file_path),
                                        "loaded_at": datetime.now().isoformat(),
                                        "file_size_bytes": os.path.getsize(file_path)
                                    })
                                    documents.append(item)
                        
                        # Si es un objeto JSON Ãºnico
                        elif isinstance(data, dict):
                            data.update({
                                "source_file": os.path.basename(file_path),
                                "loaded_at": datetime.now().isoformat(),
                                "file_size_bytes": os.path.getsize(file_path)
                            })
                            documents.append(data)
                    
                    except json.JSONDecodeError:
                        # Si no es JSON vÃ¡lido, cargar como texto
                        f.seek(0)
                        content = f.read()
                        documents.append({
                            "raw_content": content,
                            "source_file": os.path.basename(file_path),
                            "loaded_at": datetime.now().isoformat(),
                            "file_size_bytes": os.path.getsize(file_path),
                            "content_type": "raw_text"
                        })
                        
            except Exception as e:
                self.logger.error(f"Error leyendo archivo {file_path}: {e}")
        
        print(f"âœ… Cargados {len(documents)} documentos desde {len(file_paths)} archivos")
        
        # Insertar en MongoDB
        if documents:
            return self.bulk_insert_optimized(documents)
        else:
            return {"inserted_count": 0, "errors": ["No se encontraron documentos vÃ¡lidos"]}
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """
        Obtiene estadÃ­sticas de rendimiento
        
        Returns:
            EstadÃ­sticas detalladas
        """
        if self.stats["start_time"] and self.stats["end_time"]:
            duration = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()
            docs_per_second = self.stats["total_inserted"] / duration if duration > 0 else 0
            
            return {
                "total_inserted": self.stats["total_inserted"],
                "total_errors": self.stats["total_errors"],
                "batches_processed": self.stats["batches_processed"],
                "duration_seconds": duration,
                "documents_per_second": round(docs_per_second, 2),
                "batch_size_used": self.batch_size,
                "average_batch_time": round(duration / self.stats["batches_processed"], 3) if self.stats["batches_processed"] > 0 else 0
            }
        
        return self.stats

def benchmark_different_strategies():
    """FunciÃ³n para comparar diferentes estrategias de carga"""
    
    strategies = [
        {"name": "Lotes pequeÃ±os", "batch_size": 100},
        {"name": "Lotes medianos", "batch_size": 1000},
        {"name": "Lotes grandes", "batch_size": 5000},
        {"name": "Lotes muy grandes", "batch_size": 10000}
    ]
    
    print("ğŸ§ª BENCHMARK DE ESTRATEGIAS DE CARGA")
    print("=" * 50)
    
    for strategy in strategies:
        print(f"\nğŸ”„ Probando: {strategy['name']} (batch_size: {strategy['batch_size']})")
        
        loader = OptimizedBulkLoader(batch_size=strategy['batch_size'])
        
        # Simular documentos para prueba
        test_documents = [
            {"test_id": i, "data": f"documento_{i}", "timestamp": datetime.now().isoformat()}
            for i in range(1000)  # 1000 documentos de prueba
        ]
        
        start_time = time.time()
        result = loader.bulk_insert_optimized(test_documents)
        end_time = time.time()
        
        print(f"   âœ… Insertados: {result['inserted_count']} documentos")
        print(f"   â±  Tiempo: {end_time - start_time:.3f} segundos")
        print(f"   ğŸš€ Velocidad: {result['inserted_count'] / (end_time - start_time):.2f} docs/seg")

if __name__ == "__main__":
    # Conectar a MongoDB
    if mongo_connection.connect():
        print("âœ… Conectado a MongoDB")
        
        # Ejecutar benchmark
        benchmark_different_strategies()
        
        mongo_connection.disconnect()
    else:
        print("âŒ Error conectando a MongoDB")